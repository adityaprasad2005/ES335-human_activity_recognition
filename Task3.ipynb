{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 : Prompt Engineering for Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot and Few Shot Prompting :\n",
    "Zero-shot prompting involves providing a language model with a prompt or a set of instructions that allows it to generate text or perform a task without any explicit training data or labeled examples. The model is expected to generate high-quality text or perform the task accurately based solely on the prompt and its internal knowledge.\n",
    "\n",
    "Few-shot prompting is similar to zero-shot prompting, but it involves providing the model with a limited number of labeled examples or prompts that are relevant to the specific task or dataset. The model is then expected to generate high-quality text or perform the task accurately based on the few labeled examples and its internal knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 1: Demonstrate how to use Zero-Shot Learning and Few-Shot Learning to classify human activities based on the featurized accelerometer data. Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. Which method performs better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API and Models \n",
    "Groq_Token = \"\"  # Do not share this key with anyone\n",
    "\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (90000, 3)\n",
      "y.shape:  (180, 1)\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='180'\n",
       "                  max='180',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  180\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "X_train.shape:  (126, 1152)\n",
      "X_test.shape:  (54, 1152)\n"
     ]
    }
   ],
   "source": [
    "# importing the dataset\n",
    "X = pd.read_csv('data_X.csv', delimiter=\",\", header=None)\n",
    "y = pd.read_csv('data_y.csv', delimiter=\",\", header=None)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X.shape: \", X.shape)\n",
    "print(\"y.shape: \", y.shape)\n",
    "\n",
    "import tsfel\n",
    "\n",
    "ts = tsfel.get_features_by_domain()\n",
    "X_tsfel = tsfel.time_series_features_extractor(ts, X, fs=50, window_size=500)\n",
    "\n",
    "# train test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tsfel, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero shot ans:  [4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 1, 2, 4, 5]\n",
      "[4 5 6 1 2 3 4 5 6 1 1 1 2 2 3 3 4 4 5 5 6 6 1 2 3 4 5 6 1 2 3 4 5 6 1 1 2\n",
      " 2 3 3 4 4 5 5 6 6 1 2 4 5]\n",
      "Zero Shot Learning Accuracy: 0.16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Zero Shot Learning\"\"\"\n",
    "\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a machine learning classsifier model(Real Input Discrete output). \n",
    "* Based on the featurized accerlerometer data having 1152 features you have to predict the human activity.\n",
    "* Activities can be among the following: Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing, Laying.\n",
    "* activity_labels = [\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6]\n",
    "* You have predict the human activity for every row in the X_train dataset.\n",
    "\n",
    "*PS: Just give the prediction array for the given dataset without any explanation or anything above or below it.\n",
    "*PS: Ensure that the predicted array looks like [1,2,3,..,5,6] and is of the length = {X_train[:50].shape[0]}. \n",
    "\"\"\" \n",
    "\n",
    "# * The dataset is in the following format: {X}\n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3.1-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "zero_shot_answer = llm.invoke(query).content\n",
    "print(\"zero shot ans: \", zero_shot_answer)\n",
    "\n",
    "# Convert the zero shot answer to a numpy array\n",
    "str= zero_shot_answer.strip(\"[]\").split(\",\")\n",
    "str_not_null= [i for i in str if i !=\" \"]\n",
    "y_pred= np.array([int(i) for i in str_not_null])\n",
    "\n",
    "print(y_pred)\n",
    "# print(f\"len(y): {len(y)}, len(y_pred): {len(y_pred)}\")\n",
    "\n",
    "if y_train[:50].shape[0] == y_pred.shape[0]:\n",
    "    zero_shot_accuracy_score = accuracy_score(y_train[:50].flatten(), y_pred)\n",
    "    print(f\"Zero Shot Learning Accuracy: {zero_shot_accuracy_score}\")\n",
    "else:\n",
    "    print(\"The accuracy of the Zero Shot Learning model could not be calculated since the model did not provide the prediction array in the correct format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few shot answer:  [4, 6, 5, 4, 3]\n",
      "Few Shot Learning Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Few Shot Learning\"\"\"\n",
    "\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a machine learning classsifier model(Real Input Discrete output). \n",
    "* Based on the tsfel-featurized accerlerometer data having 1152 columns  you have to predict the human activity.\n",
    "* Activities can be among the following: Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing, Laying.\n",
    "* activity_labels = [\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6]\n",
    "* You have predict the human activity for every row input in the X_test dataset and output the corresponding activity_label.\n",
    "\n",
    "* You have been trained on the following dataset:\n",
    "* Training Dataset: {X_train[:20]}\n",
    "* Training Labels: {y_train[:20]}\n",
    "\n",
    "* The test dataset is in the following format: {X_test[:5]}\n",
    "\n",
    "*PS: Only give out the prediction array for the given dataset without any explanation without anything above or below it.\n",
    "*PS: Ensure that the prediction array is of the same length as the test dataset.\n",
    "*PS: Ensure that the prediction looks like this: [1,2,..,5,6] of length {X_test[:5].shape[0]}\n",
    "\"\"\" \n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "few_shot_answer = llm.invoke(query).content\n",
    "print(\"few shot answer: \", few_shot_answer)\n",
    "\n",
    "# convert the few_shot_answer to a numpy array\n",
    "str= few_shot_answer.strip(\"[]\").split(\",\")\n",
    "str_not_null= [i for i in str if i !=\" \"]\n",
    "y_pred= np.array([int(i) for i in str_not_null])\n",
    "\n",
    "# print(f\"len(y_test): {len(y_test)}, len(y_pred): {len(y_pred)}\")\n",
    "\n",
    "if y_test[:5].shape[0] == y_pred.shape[0]:\n",
    "    few_shot_accuracy_score = accuracy_score(y_test[:5].flatten(), y_pred)\n",
    "    print(f\"Few Shot Learning Accuracy: {few_shot_accuracy_score}\")\n",
    "else:\n",
    "    print(\"The accuracy of the Few Shot Learning model could not be calculated since the model did not provide the prediction array in the correct format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. \n",
    "Which method performs better? Why?\n",
    "In general, Few-Shot Learning should perform better than Zero-Shot Learning because it has\n",
    " seen some examples of the target classes, allowing it to adapt better to the specific task. \n",
    " However, the performance difference can vary depending on the complexity of the task and the \n",
    " quality of the few-shot examples provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 2: Quantitatively compare the accuracy of Few-Shot Learning with Decision Trees (You may use a subset of the test set if you encounter rate-limiting issues). Which method performs better? Why?\n",
    "\n",
    "Few Shot Learning Accuracy: 0.67\n",
    "Decision Tree model Accuracy: 0.71\n",
    "Decision Tree Classifier works better than Few Shot LLM model for this dataset \n",
    "\n",
    "But both the models have comparable accuracy.\n",
    "      Some Advantages of Few Shot LLM model are:\n",
    "        1. Leverage Large Amount of data from the internet\n",
    "        2. Have a better physical understanding of the data and its underlying meaning.\n",
    "        3. Can be used for a wide range of general tasks.\n",
    "      \n",
    "      Some Advantages of Decision Tree Classifier are:\n",
    "        1. Hpyerparameters can be tuned to improve the accuracy.\n",
    "        2. Works best for complex tasks/datasets with a lot of non-standard features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 3: What are the limitations of Zero-Shot Learning and Few-Shot Learning in the context of classifying human activities based on featurized accelerometer data?\n",
    "Zero-Shot Learning (ZSL) and Few-Shot Learning (FSL) are powerful techniques, \n",
    "    but they come with their own set of limitations, especially in the context of classifying human activities\n",
    "    based on featurized accelerometer data.\n",
    "\n",
    " Zero-Shot Learning:\n",
    "    1. Lack of Training Data: ZSL models rely on general knowledge and may not have specific information\n",
    "    about the target classes, leading to lower accuracy.\n",
    "    2. Contextual Understanding: ZSL models may struggle to understand the context of activities, \n",
    "    especially if the activities are complex or involve subtle differences. \n",
    "    3. Feature data: ZSL models require well-defined standard features whose data is widely available. \n",
    "    ZSL models dont perform well with new domain-specific non-standard features.\n",
    "\n",
    " Few-Shot Learning:\n",
    "    1. Overfitting: FSL models may overfit the limited training data, especially if the data is noisy or unrepresentative.\n",
    "    2. Class Imbalance: FSL models may struggle with class imbalance, as the few-shot classes may not have enough representative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4: What does the model classify when given input from an entirely new activity that it hasn't seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerometer data for the new activity: [[1. 1. 1. ... 1. 1. 1.]]\n",
      "The model classified the new activity as: WALKING_UPSTAIRS\n"
     ]
    }
   ],
   "source": [
    "# Let us assume that the new activity is \"Jogging\"\n",
    "new_activity_data= np.ones((1, 1152))\n",
    "\n",
    "\"\"\"Few Shot Learning\"\"\"\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a machine learning classsifier model(Real Input Discrete output). \n",
    "* Based on the tsfel-featurized accerlerometer data you have to predict the human activity.\n",
    "* Activities can be among the following: Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing, Laying.\n",
    "* activity_labels = [\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6]\n",
    "\n",
    "* You have been trained on the following dataset:\n",
    "* Training Dataset: {X_train[:20]}\n",
    "* Training Labels: {y_train[:20]}\n",
    "\n",
    "* The test dataset is in the following format: {new_activity_data}\n",
    "\n",
    "* You have to predict the human activity for the given test dataset.\n",
    "* Kindly output only the activity name and nothing below or above it.\n",
    "\"\"\" \n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "few_shot_answer = llm.invoke(query).content\n",
    "\n",
    "print(f\"Accelerometer data for the new activity: {new_activity_data}\")\n",
    "print(f\"The model classified the new activity as: {few_shot_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 5: Test the model with random data (ensuring the data has the same dimensions and range as the previous input) and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few shot answer:  [4, 3, 2, 5, 6]\n",
      "Few Shot Learning Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Few Shot Learning and testing with random data\"\"\"\n",
    "\n",
    "X_test = np.random.rand(100, 1152)\n",
    "y_test = np.random.randint(1, 7, 100)\n",
    "\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a machine learning classsifier model(Real Input Discrete output). \n",
    "* Based on the tsfel-featurized accerlerometer data having 1152 columns  you have to predict the human activity.\n",
    "* Activities can be among the following: Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing, Laying.\n",
    "* activity_labels = [\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6]\n",
    "* You have predict the human activity for every row input in the X_test dataset and output the corresponding activity_label.\n",
    "\n",
    "* You have been trained on the following dataset:\n",
    "* Training Dataset: {X_train[:20]}\n",
    "* Training Labels: {y_train[:20]}\n",
    "\n",
    "* The test dataset is in the following format: {X_test[:5]}\n",
    "\n",
    "*PS: Only give out the prediction array for the given dataset without any explanation without anything above or below it.\n",
    "*PS: Ensure that the prediction array is of the same length as the test dataset.\n",
    "*PS: Ensure that the prediction looks like this: [1,2,..,5,6] of length {X_test[:5].shape[0]}\n",
    "\"\"\" \n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "few_shot_answer = llm.invoke(query).content\n",
    "print(\"few shot answer: \", few_shot_answer)\n",
    "\n",
    "# convert the few_shot_answer to a numpy array\n",
    "str= few_shot_answer.strip(\"[]\").split(\",\")\n",
    "str_not_null= [i for i in str if i !=\" \"]\n",
    "y_pred= np.array([int(i) for i in str_not_null])\n",
    "\n",
    "# print(f\"len(y_test): {len(y_test)}, len(y_pred): {len(y_pred)}\")\n",
    "\n",
    "if y_test[:5].shape[0] == y_pred.shape[0]:\n",
    "    few_shot_accuracy_score = accuracy_score(y_test[:5].flatten(), y_pred)\n",
    "    print(f\"Few Shot Learning Accuracy: {few_shot_accuracy_score}\")\n",
    "else:\n",
    "    print(\"The accuracy of the Few Shot Learning model could not be calculated since the model did not provide the prediction array in the correct format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
